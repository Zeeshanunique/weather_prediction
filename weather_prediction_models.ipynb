{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# # Weather Prediction Model Training\n",
    "# \n",
    "# This notebook allows you to train different models for air pollution prediction. Each code block trains a specific type of model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## Setup and Data Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from models import LSTMModel, ANNModel, RandomForestModel, LinearRegressionModel, MultipleLinearRegressionModel\n",
    "from models.preprocessing import DataPreprocessor\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create directory for saved models\n",
    "models_dir = 'saved_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Set prediction length\n",
    "prediction_length = '1day'  # '1day', '1week', or '1month'\n",
    "\n",
    "# Define sequence length based on prediction length\n",
    "if prediction_length == '1day':\n",
    "    sequence_length = 24  # 24 hours\n",
    "elif prediction_length == '1week':\n",
    "    sequence_length = 24 * 7  # 7 days\n",
    "elif prediction_length == '1month':\n",
    "    sequence_length = 24 * 30  # 30 days\n",
    "\n",
    "# Load and prepare data\n",
    "def load_all_data(data_path='data'):\n",
    "    # Get all station data files\n",
    "    data_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "    if not data_files:\n",
    "        print(f\"No data files found in {data_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(data_files)} data files\")\n",
    "    \n",
    "    # Load and combine all data\n",
    "    combined_data = []\n",
    "    station_names = []\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        station_name = os.path.basename(file_path).split('.')[0]\n",
    "        station_names.append(station_name)\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add station column\n",
    "            df['station'] = station_name\n",
    "            \n",
    "            combined_data.append(df)\n",
    "            print(f\"Loaded data for station: {station_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {station_name}: {e}\")\n",
    "    \n",
    "    if not combined_data:\n",
    "        print(\"No data could be loaded. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    all_data = pd.concat(combined_data, ignore_index=True)\n",
    "    print(f\"Combined data shape: {all_data.shape}\")\n",
    "    \n",
    "    # Get all pollutants\n",
    "    pollutant_columns = [col for col in all_data.columns if col not in \n",
    "                         ['Date', 'station', 'AMB_TEMP', 'RH', 'WIND_SPEED', 'RAINFALL']]\n",
    "    \n",
    "    print(f\"Detected pollutants: {pollutant_columns}\")\n",
    "    \n",
    "    # Create one-hot encoding for stations\n",
    "    all_data = pd.get_dummies(all_data, columns=['station'], prefix='station')\n",
    "    \n",
    "    return all_data, pollutant_columns\n",
    "\n",
    "# Data cleaning function to handle NaN and Inf values\n",
    "def clean_data_for_training(X_data, y_data):\n",
    "    \"\"\"Clean data by removing or replacing NaN and Inf values.\"\"\"\n",
    "    # Check for NaN values\n",
    "    if np.isnan(X_data).any() or np.isnan(y_data).any():\n",
    "        print(\"Warning: NaN values detected in the data. Replacing with zeros.\")\n",
    "        X_data = np.nan_to_num(X_data, nan=0.0)\n",
    "        y_data = np.nan_to_num(y_data, nan=0.0)\n",
    "    \n",
    "    # Check for infinite values\n",
    "    if np.isinf(X_data).any() or np.isinf(y_data).any():\n",
    "        print(\"Warning: Infinite values detected in the data. Replacing with large values.\")\n",
    "        X_data = np.clip(X_data, -1e15, 1e15)\n",
    "        y_data = np.clip(y_data, -1e15, 1e15)\n",
    "    \n",
    "    return X_data, y_data\n",
    "\n",
    "# Prepare training data for all pollutants\n",
    "def prepare_training_data(all_data, pollutant_columns, sequence_length):\n",
    "    preprocessor = DataPreprocessor()\n",
    "    combined_X = {}\n",
    "    combined_y = {}\n",
    "    \n",
    "    for pollutant in pollutant_columns:\n",
    "        print(f\"Preparing data for pollutant: {pollutant}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        try:\n",
    "            X_train, y_train, X_test, y_test = preprocessor.prepare_data(\n",
    "                all_data, \n",
    "                target_column=pollutant,\n",
    "                sequence_length=sequence_length,\n",
    "                include_all_features=True\n",
    "            )\n",
    "            \n",
    "            if pollutant not in combined_X:\n",
    "                combined_X[pollutant] = []\n",
    "                combined_y[pollutant] = []\n",
    "            \n",
    "            # Only add data if we have valid training samples\n",
    "            if len(X_train) > 0:\n",
    "                # Clean data before adding\n",
    "                X_train, y_train = clean_data_for_training(X_train, y_train)\n",
    "                combined_X[pollutant].append(X_train)\n",
    "                combined_y[pollutant].append(y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing data for {pollutant}: {e}\")\n",
    "    \n",
    "    return combined_X, combined_y\n",
    "\n",
    "# Load data\n",
    "all_data, pollutant_columns = load_all_data()\n",
    "\n",
    "# Prepare training data\n",
    "combined_X, combined_y = prepare_training_data(all_data, pollutant_columns, sequence_length)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## 1. Train LSTM Model\n",
    "# \n",
    "# Long Short-Term Memory networks are a type of recurrent neural network well-suited for sequence prediction problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_lstm_model(combined_X, combined_y, sequence_length, all_data):\n",
    "    \"\"\"Train a universal LSTM model.\"\"\"\n",
    "    # Combine all data for LSTM\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for pollutant in combined_X:\n",
    "        for X_data, y_data in zip(combined_X[pollutant], combined_y[pollutant]):\n",
    "            # LSTM expects 3D input\n",
    "            all_X.append(X_data)\n",
    "            all_y.append(y_data)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    if all_X and all_y:\n",
    "        X_concat = np.concatenate(all_X)\n",
    "        y_concat = np.concatenate(all_y)\n",
    "        \n",
    "        # Clean data again to ensure no NaNs or Infs\n",
    "        X_concat, y_concat = clean_data_for_training(X_concat, y_concat)\n",
    "        \n",
    "        # Normalize data to range [0, 1] for better numerical stability\n",
    "        X_mean = np.mean(X_concat, axis=(0, 1), keepdims=True)\n",
    "        X_std = np.std(X_concat, axis=(0, 1), keepdims=True) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "        X_concat = (X_concat - X_mean) / X_std\n",
    "        \n",
    "        # Get the actual input dimension from the data\n",
    "        actual_input_dim = X_concat.shape[2]\n",
    "        \n",
    "        # Initialize LSTM model with gradient clipping\n",
    "        try:\n",
    "            # Create model - note: using parameters that match the actual implementation\n",
    "            print(\"Creating LSTM model with normalized data...\")\n",
    "            lstm_model = LSTMModel(\n",
    "                input_shape=(sequence_length, actual_input_dim),\n",
    "                output_length=sequence_length\n",
    "            )\n",
    "            \n",
    "            # Lower batch size for more stability\n",
    "            batch_size = 32\n",
    "            \n",
    "            # Train model with error handling\n",
    "            try:\n",
    "                print(\"Training LSTM model with normalized data...\")\n",
    "                # The train_model method already has early stopping built in\n",
    "                lstm_model.train_model(\n",
    "                    X_concat, \n",
    "                    y_concat, \n",
    "                    epochs=50, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2  # This parameter is supported\n",
    "                )\n",
    "                \n",
    "                # Save model\n",
    "                lstm_path, lstm_meta_path = lstm_model.save(os.path.join(models_dir, f\"lstm_universal_{prediction_length}\"))\n",
    "                print(f\"LSTM model saved to {lstm_path}\")\n",
    "                \n",
    "                return lstm_model\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                print(\"Trying with a simpler model configuration...\")\n",
    "                \n",
    "                # Try with a simpler configuration - use standard parameters only\n",
    "                # Create a new model with the same architecture but different hyperparameters\n",
    "                print(\"Creating simpler LSTM model...\")\n",
    "                \n",
    "                # Custom LSTM implementation with simpler params\n",
    "                lstm_model = LSTMModel(\n",
    "                    input_shape=(sequence_length, actual_input_dim),\n",
    "                    output_length=sequence_length\n",
    "                )\n",
    "                \n",
    "                # Customize the LSTM model with simpler architecture\n",
    "                # Directly modify relevant attributes instead of using constructor params\n",
    "                # Reduce complexity by using smaller network, lower learning rate\n",
    "                lstm_model.dropout1 = nn.Dropout(0.1)  # Lower dropout rate\n",
    "                lstm_model.dropout2 = nn.Dropout(0.1)\n",
    "                \n",
    "                # Reduce batch size further for stability\n",
    "                batch_size = 16\n",
    "                \n",
    "                lstm_model.train_model(\n",
    "                    X_concat, \n",
    "                    y_concat, \n",
    "                    epochs=30,  # Fewer epochs\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2  # Already supported\n",
    "                )\n",
    "                \n",
    "                lstm_path, lstm_meta_path = lstm_model.save(os.path.join(models_dir, f\"lstm_simple_{prediction_length}\"))\n",
    "                print(f\"Simplified LSTM model saved to {lstm_path}\")\n",
    "                \n",
    "                return lstm_model\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create LSTM model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    raise ValueError(\"No data available for LSTM training\")\n",
    "\n",
    "# Train LSTM model\n",
    "try:\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    lstm_model = train_lstm_model(combined_X, combined_y, sequence_length, all_data)\n",
    "    print(\"LSTM model training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error training LSTM model: {e}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## 2. Train ANN Model\n",
    "# \n",
    "# Artificial Neural Networks are simple feed-forward networks that can learn complex patterns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_ann_model(combined_X, combined_y, all_data, sequence_length):\n",
    "    \"\"\"Train a universal ANN model.\"\"\"\n",
    "    # Combine all data for ANN\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for pollutant in combined_X:\n",
    "        for X_data, y_data in zip(combined_X[pollutant], combined_y[pollutant]):\n",
    "            # ANN expects 2D input, use the last time step\n",
    "            all_X.append(X_data[:, -1, :])\n",
    "            all_y.append(y_data)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    if all_X and all_y:\n",
    "        X_concat = np.concatenate(all_X)\n",
    "        y_concat = np.concatenate(all_y)\n",
    "        \n",
    "        # Clean data\n",
    "        X_concat, y_concat = clean_data_for_training(X_concat, y_concat)\n",
    "        \n",
    "        # Normalize data\n",
    "        X_mean = np.mean(X_concat, axis=0, keepdims=True)\n",
    "        X_std = np.std(X_concat, axis=0, keepdims=True) + 1e-8\n",
    "        X_concat = (X_concat - X_mean) / X_std\n",
    "        \n",
    "        # Get the actual input dimension from the data\n",
    "        actual_input_dim = X_concat.shape[1]\n",
    "        \n",
    "        try:\n",
    "            # Initialize ANN model\n",
    "            print(\"Creating ANN model...\")\n",
    "            ann_model = ANNModel(\n",
    "                input_shape=actual_input_dim,\n",
    "                output_length=sequence_length\n",
    "            )\n",
    "            \n",
    "            # Train model with error handling\n",
    "            try:\n",
    "                print(\"Training ANN model with normalized data...\")\n",
    "                # ANNModel already has early stopping built in\n",
    "                ann_model.train_model(\n",
    "                    X_concat, \n",
    "                    y_concat, \n",
    "                    epochs=50, \n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2\n",
    "                )\n",
    "                \n",
    "                # Save model\n",
    "                ann_path, ann_meta_path = ann_model.save(os.path.join(models_dir, f\"ann_universal_{prediction_length}\"))\n",
    "                print(f\"ANN model saved to {ann_path}\")\n",
    "                \n",
    "                return ann_model\n",
    "            except Exception as e:\n",
    "                print(f\"Error during ANN training: {e}\")\n",
    "                print(\"Trying with a simpler ANN configuration...\")\n",
    "                \n",
    "                # Try with a simpler model by directly modifying the model\n",
    "                print(\"Creating simpler ANN model...\")\n",
    "                \n",
    "                # Create a new ANN model with the same architecture but simpler configuration\n",
    "                ann_model = ANNModel(\n",
    "                    input_shape=actual_input_dim,\n",
    "                    output_length=sequence_length\n",
    "                )\n",
    "                \n",
    "                # Modify the model to be simpler by replacing the sequential model\n",
    "                ann_model.model = nn.Sequential(\n",
    "                    nn.Linear(actual_input_dim, 64),  # Smaller first layer\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.1),  # Lower dropout\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, sequence_length)\n",
    "                )\n",
    "                \n",
    "                # Train with reduced batch size\n",
    "                ann_model.train_model(\n",
    "                    X_concat, \n",
    "                    y_concat, \n",
    "                    epochs=30, \n",
    "                    batch_size=16,\n",
    "                    validation_split=0.2\n",
    "                )\n",
    "                \n",
    "                ann_path, ann_meta_path = ann_model.save(os.path.join(models_dir, f\"ann_simple_{prediction_length}\"))\n",
    "                print(f\"Simplified ANN model saved to {ann_path}\")\n",
    "                \n",
    "                return ann_model\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create ANN model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    raise ValueError(\"No data available for ANN training\")\n",
    "\n",
    "# Train ANN model\n",
    "try:\n",
    "    print(\"\\nTraining ANN model...\")\n",
    "    ann_model = train_ann_model(combined_X, combined_y, all_data, sequence_length)\n",
    "    print(\"ANN model training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error training ANN model: {e}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## 3. Train Random Forest Model\n",
    "# \n",
    "# Random Forest is an ensemble learning method that operates by constructing multiple decision trees."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_random_forest_model(combined_X, combined_y, sequence_length):\n",
    "    \"\"\"Train a universal Random Forest model.\"\"\"\n",
    "    # Combine all data for RF\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for pollutant in combined_X:\n",
    "        for X_data, y_data in zip(combined_X[pollutant], combined_y[pollutant]):\n",
    "            # RF expects 2D input, flatten the sequence\n",
    "            all_X.append(X_data.reshape(X_data.shape[0], -1))\n",
    "            all_y.append(y_data)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    if all_X and all_y:\n",
    "        X_concat = np.concatenate(all_X)\n",
    "        y_concat = np.concatenate(all_y)\n",
    "        \n",
    "        # Clean data\n",
    "        X_concat, y_concat = clean_data_for_training(X_concat, y_concat)\n",
    "        \n",
    "        try:\n",
    "            # Initialize RF model\n",
    "            print(\"Creating Random Forest model...\")\n",
    "            rf_model = RandomForestModel(n_estimators=100, max_depth=None)\n",
    "            \n",
    "            # Train model - note: RandomForestModel uses train() method, not train_model()\n",
    "            print(\"Training Random Forest model...\")\n",
    "            rf_model.train(X_concat, y_concat)\n",
    "            \n",
    "            # Save model\n",
    "            rf_path, rf_meta_path = rf_model.save(os.path.join(models_dir, f\"random_forest_universal_{prediction_length}\"))\n",
    "            print(f\"Random Forest model saved to {rf_path}\")\n",
    "            \n",
    "            return rf_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Random Forest training: {e}\")\n",
    "            print(\"Trying with reduced complexity...\")\n",
    "            \n",
    "            # Try with reduced complexity\n",
    "            print(\"Creating simpler Random Forest model...\")\n",
    "            rf_model = RandomForestModel(n_estimators=50, max_depth=10)\n",
    "            rf_model.train(X_concat, y_concat)\n",
    "            \n",
    "            rf_path, rf_meta_path = rf_model.save(os.path.join(models_dir, f\"random_forest_simple_{prediction_length}\"))\n",
    "            print(f\"Simplified Random Forest model saved to {rf_path}\")\n",
    "            \n",
    "            return rf_model\n",
    "    \n",
    "    raise ValueError(\"No data available for Random Forest training\")\n",
    "\n",
    "# Train Random Forest model\n",
    "try:\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    rf_model = train_random_forest_model(combined_X, combined_y, sequence_length)\n",
    "    print(\"Random Forest model training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error training Random Forest model: {e}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## 4. Train Linear Regression Model\n",
    "# \n",
    "# Linear Regression is a simple approach that models the relationship between variables using a linear predictor function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_linear_regression_model(combined_X, combined_y, sequence_length):\n",
    "    \"\"\"Train a universal Linear Regression model.\"\"\"\n",
    "    # Combine all data for Linear Regression\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for pollutant in combined_X:\n",
    "        for X_data, y_data in zip(combined_X[pollutant], combined_y[pollutant]):\n",
    "            # LR expects 2D input, use the last time step\n",
    "            all_X.append(X_data[:, -1, :])\n",
    "            all_y.append(y_data)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    if all_X and all_y:\n",
    "        X_concat = np.concatenate(all_X)\n",
    "        y_concat = np.concatenate(all_y)\n",
    "        \n",
    "        # Clean data\n",
    "        X_concat, y_concat = clean_data_for_training(X_concat, y_concat)\n",
    "        \n",
    "        try:\n",
    "            # Initialize LR model\n",
    "            print(\"Creating Linear Regression model...\")\n",
    "            lr_model = LinearRegressionModel()\n",
    "            \n",
    "            # Train model - using train() method instead of train_model()\n",
    "            print(\"Training Linear Regression model...\")\n",
    "            lr_model.train(X_concat, y_concat)\n",
    "            \n",
    "            # Save model\n",
    "            lr_path, lr_meta_path = lr_model.save(os.path.join(models_dir, f\"linear_regression_universal_{prediction_length}\"))\n",
    "            print(f\"Linear Regression model saved to {lr_path}\")\n",
    "            \n",
    "            return lr_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Linear Regression training: {e}\")\n",
    "            # Linear regression is already simple, but we can try to handle failures\n",
    "            # by reducing data size if needed\n",
    "            if len(X_concat) > 10000:\n",
    "                print(\"Reducing data size for Linear Regression...\")\n",
    "                indices = np.random.choice(len(X_concat), 10000, replace=False)\n",
    "                X_reduced = X_concat[indices]\n",
    "                y_reduced = y_concat[indices]\n",
    "                \n",
    "                lr_model = LinearRegressionModel()\n",
    "                lr_model.train(X_reduced, y_reduced)\n",
    "                \n",
    "                lr_path, lr_meta_path = lr_model.save(os.path.join(models_dir, f\"linear_regression_reduced_{prediction_length}\"))\n",
    "                print(f\"Reduced Linear Regression model saved to {lr_path}\")\n",
    "                \n",
    "                return lr_model\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise ValueError(\"No data available for Linear Regression training\")\n",
    "\n",
    "# Train Linear Regression model\n",
    "try:\n",
    "    print(\"\\nTraining Linear Regression model...\")\n",
    "    lr_model = train_linear_regression_model(combined_X, combined_y, sequence_length)\n",
    "    print(\"Linear Regression model training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error training Linear Regression model: {e}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## 5. Train Multiple Linear Regression (MLR) Model\n",
    "# \n",
    "# Multiple Linear Regression extends simple linear regression to include multiple input variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_mlr_model(combined_X, combined_y, sequence_length):\n",
    "    \"\"\"Train a universal MLR model.\"\"\"\n",
    "    # Combine all data for MLR\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    for pollutant in combined_X:\n",
    "        for X_data, y_data in zip(combined_X[pollutant], combined_y[pollutant]):\n",
    "            # MLR expects 2D input, flatten the sequence\n",
    "            all_X.append(X_data.reshape(X_data.shape[0], -1))\n",
    "            all_y.append(y_data)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    if all_X and all_y:\n",
    "        X_concat = np.concatenate(all_X)\n",
    "        y_concat = np.concatenate(all_y)\n",
    "        \n",
    "        # Clean data\n",
    "        X_concat, y_concat = clean_data_for_training(X_concat, y_concat)\n",
    "        \n",
    "        try:\n",
    "            # Initialize MLR model\n",
    "            print(\"Creating Multiple Linear Regression model...\")\n",
    "            mlr_model = MultipleLinearRegressionModel()\n",
    "            \n",
    "            # Train model - using train() method instead of train_model()\n",
    "            print(\"Training Multiple Linear Regression model...\")\n",
    "            mlr_model.train(X_concat, y_concat)\n",
    "            \n",
    "            # Save model\n",
    "            mlr_path, mlr_meta_path = mlr_model.save(os.path.join(models_dir, f\"mlr_universal_{prediction_length}\"))\n",
    "            print(f\"MLR model saved to {mlr_path}\")\n",
    "            \n",
    "            return mlr_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error during MLR training: {e}\")\n",
    "            # Like with LR, reduce data size if needed\n",
    "            if len(X_concat) > 10000:\n",
    "                print(\"Reducing data size for MLR...\")\n",
    "                indices = np.random.choice(len(X_concat), 10000, replace=False)\n",
    "                X_reduced = X_concat[indices]\n",
    "                y_reduced = y_concat[indices]\n",
    "                \n",
    "                mlr_model = MultipleLinearRegressionModel()\n",
    "                mlr_model.train(X_reduced, y_reduced)\n",
    "                \n",
    "                mlr_path, mlr_meta_path = mlr_model.save(os.path.join(models_dir, f\"mlr_reduced_{prediction_length}\"))\n",
    "                print(f\"Reduced MLR model saved to {mlr_path}\")\n",
    "                \n",
    "                return mlr_model\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise ValueError(\"No data available for MLR training\")\n",
    "\n",
    "# Train MLR model\n",
    "try:\n",
    "    print(\"\\nTraining MLR model...\")\n",
    "    mlr_model = train_mlr_model(combined_X, combined_y, sequence_length)\n",
    "    print(\"MLR model training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error training MLR model: {e}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ## Running the Web Interface\n",
    "# \n",
    "# After training the models, you can run the web interface to interact with them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the Flask application\n",
    "print(\"To start the web interface, run the following command in your terminal:\")\n",
    "print(\"python app.py\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}